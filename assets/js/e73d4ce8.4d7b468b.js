"use strict";(globalThis.webpackChunktemp_docusaurus_init=globalThis.webpackChunktemp_docusaurus_init||[]).push([[7338],{6604:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"vision-language-action/voice-to-action","title":"Chapter 11: Voice-to-Action","description":"Module 4 hours | Difficulty: Advanced","source":"@site/docs/04-vision-language-action/11-voice-to-action.md","sourceDirName":"04-vision-language-action","slug":"/vision-language-action/voice-to-action","permalink":"/AI-Humanoid-Book/docs/vision-language-action/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/specifykit/ai-native-book/tree/main/docs/04-vision-language-action/11-voice-to-action.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{},"sidebar":"mainSidebar","previous":{"title":"Module 4: Vision-Language-Action","permalink":"/AI-Humanoid-Book/docs/vision-language-action/"},"next":{"title":"Chapter 12: LLM Cognitive Planning","permalink":"/AI-Humanoid-Book/docs/vision-language-action/llm-cognitive-planning"}}');var t=i(4848),s=i(8453);const r={},a="Chapter 11: Voice-to-Action",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"What You&#39;ll Build",id:"what-youll-build",level:2},{value:"Introduction: Conversing with Robots",id:"introduction-conversing-with-robots",level:2},{value:"Core Concepts: Automatic Speech Recognition and Whisper",id:"core-concepts-automatic-speech-recognition-and-whisper",level:2},{value:"1. Automatic Speech Recognition (ASR)",id:"1-automatic-speech-recognition-asr",level:3},{value:"2. OpenAI Whisper",id:"2-openai-whisper",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function d(e){const n={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-11-voice-to-action",children:"Chapter 11: Voice-to-Action"})}),"\n",(0,t.jsx)(n.admonition,{title:"Chapter Info",type:"info",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Module"}),": Vision-Language-Action | ",(0,t.jsx)(n.strong,{children:"Duration"}),": 4 hours | ",(0,t.jsx)(n.strong,{children:"Difficulty"}),": Advanced"]})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this chapter, you'll be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Comprehend Automatic Speech Recognition (ASR) role in human-robot interaction."}),"\n",(0,t.jsx)(n.li,{children:"Integrate OpenAI Whisper for high-accuracy speech-to-text conversion."}),"\n",(0,t.jsx)(n.li,{children:"Create ROS 2 nodes processing audio input and publishing text commands."}),"\n",(0,t.jsx)(n.li,{children:"Master basic Natural Language Understanding (NLU) for extracting robotic intent from voice commands."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completion of Module 3 (Chapters 8-10), with functional simulated robot in Isaac Sim."}),"\n",(0,t.jsx)(n.li,{children:"Basic audio processing concepts understanding."}),"\n",(0,t.jsx)(n.li,{children:"OpenAI API key (or access to local Whisper model)."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"what-youll-build",children:"What You'll Build"}),"\n",(0,t.jsx)(n.p,{children:"This chapter constructs foundational voice control system for robots, involving:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Setting up microphone and audio capture."}),"\n",(0,t.jsx)(n.li,{children:"Implementing ROS 2 node capturing audio and sending to OpenAI Whisper."}),"\n",(0,t.jsx)(n.li,{children:"Receiving transcribed text from Whisper and publishing to ROS 2 topic."}),"\n",(0,t.jsx)(n.li,{children:"Basic ROS 2 node interpreting simple text commands."}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"introduction-conversing-with-robots",children:"Introduction: Conversing with Robots"}),"\n",(0,t.jsxs)(n.p,{children:['Envision a future where you simply tell robots what to do using natural language, just as you\'d instruct another person. "Robot, pick up the red cube and place it on the table." This seemingly simple interaction involves complex chains of perception, cognition, and action, starting with understanding human speech. The ability for robots to interpret voice commands, often termed ',(0,t.jsx)(n.strong,{children:"Voice-to-Action"}),", is a critical component of intuitive human-robot interaction and key enabler for advanced Physical AI systems."]}),"\n",(0,t.jsxs)(n.p,{children:["Until recently, robust and accurate Automatic Speech Recognition (ASR) was challenging, especially in noisy environments or for diverse accents. However, deep learning advancements have led to breakthroughs, with models like ",(0,t.jsx)(n.strong,{children:"OpenAI Whisper"})," achieving near human-level accuracy across wide ranges of languages and domains. Integrating such powerful ASR capabilities into robotic systems transforms how humans can interact with intelligent counterparts."]}),"\n",(0,t.jsx)(n.p,{children:'This chapter guides you through bridging the gap between spoken language and robotic action. We begin exploring ASR principles and its specific challenges in robotics contexts. You then master integrating OpenAI Whisper into ROS 2-based robotic systems, setting up pipelines to capture audio, send for transcription, and receive text commands. Finally, we introduce basic Natural Language Understanding (NLU) techniques extracting robot\'s intended action from transcribed text, laying groundwork for cognitive planning discussed in next chapter. By chapter\'s end, your simulated robot will "hear" and "understand" simple voice instructions, bringing it closer to truly autonomous and intelligent behavior.'}),"\n",(0,t.jsx)(n.h2,{id:"core-concepts-automatic-speech-recognition-and-whisper",children:"Core Concepts: Automatic Speech Recognition and Whisper"}),"\n",(0,t.jsx)(n.h3,{id:"1-automatic-speech-recognition-asr",children:"1. Automatic Speech Recognition (ASR)"}),"\n",(0,t.jsx)(n.p,{children:"ASR is the process converting spoken words into text. For robotics, ASR systems need to be:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accurate"}),": Misinterpretations can lead to incorrect or dangerous robot actions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robust"}),": Able to handle variations in speaker, accent, background noise, and speech speed."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Low Latency"}),": For real-time interaction, transcription needs happening quickly."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Modern ASR, especially with deep learning, typically uses end-to-end neural networks learning directly from audio-text pairs."}),"\n",(0,t.jsx)(n.h3,{id:"2-openai-whisper",children:"2. OpenAI Whisper"}),"\n",(0,t.jsx)(n.p,{children:"OpenAI Whisper is a general-purpose ASR model achieving high accuracy and robustness across many languages. Trained on massive datasets of diverse audio and corresponding transcriptions, making it highly generalized."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Whisper features for robotics:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multilingual"}),": Can transcribe in many languages and translate non-English speech into English."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness"}),": Handles various audio conditions, including background noise and different recording qualities."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transformer-based"}),": Utilizes Transformer architecture excelling at sequence-to-sequence tasks."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"API or Local Models"}),": Available via easy-to-use API or can run locally. For real-time robotics, local deployment often preferred minimizing latency and ensuring privacy."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This chapter built foundational Voice-to-Action pipeline for robots:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"You comprehended ASR principles and its role in human-robot interaction."}),"\n",(0,t.jsx)(n.li,{children:"You integrated OpenAI Whisper for speech-to-text conversion."}),"\n",(0,t.jsx)(n.li,{children:"You created ROS 2 nodes for audio capture and text command publishing."}),"\n",(0,t.jsx)(n.li,{children:"You implemented basic NLU interpreting voice commands."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This enables robots understanding spoken language, paving way for more natural and intuitive interactions."}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:'The next chapter, "LLM Cognitive Planning," teaches leveraging immense power of Large Language Models transforming simple text commands into complex, multi-step action plans for robots.'}),"\n",(0,t.jsxs)(n.p,{children:["\u27a1\ufe0f Continue to ",(0,t.jsx)(n.a,{href:"/AI-Humanoid-Book/docs/vision-language-action/llm-cognitive-planning",children:"Chapter 12: LLM Cognitive Planning"})]}),"\n",(0,t.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/audio",children:"OpenAI Whisper API Documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://pypi.org/project/SpeechRecognition/",children:"SpeechRecognition Library Documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://developer.nvidia.com/riva",children:"NVIDIA Riva (for on-device ASR)"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);